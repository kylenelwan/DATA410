---
title: "DATA 410 Linear Casual atemp"
author: "Kyle Nelwan"
date: "04/04/2022"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MPV)
library(MASS)
library(bestglm)
library(glmnet)
library(regclass)
```

## DATABASE SETUP

```{r initialize}
df <- read.csv("hour.csv")
df<- subset(df, select = -c(instant,cnt,dteday))
```

## INITIAL DATABASE ASSESSMENT

```{r model 1 casual initial assessment}
model <- lm(casual ~. - registered, data = df)
summary(model)
plot(model)
```
First, we regress the number of casual users with all the available variables. At first glance, we could see that most of variables are highly significant. However, notice that our adjusted R-squared is less than 0.5. Therefore, even though our model is significant, it can only explain less than half of the data.

The residuals vs fitted diagnostic plot has a clear pattern. Our data set seems to have a quadratic relationship.
The Normal Q-Q plot shows that our model is highly right skewed. This is a huge problem for us. Notice that those rightly skewed data have a very large value of standardized residuals. Hence, the this original linear model is flawed.
The residuals vs leverage plot tells us that none of our data are influential points but have high residuals as stated before.

## MULTICOLLINEARITY ASSESSMENT

```{r model 1 casual VIF}
VIF(model)
```
We can see that there exist a high VIF value between temp and atemp. This make sense since both variable indicates the value of the hourly temperature. Therefore, we need to remove one of those variables.
Also, there seems to be a relatively high VIF value between season and month. This make sense since certain months always have the same season. Hence, we remove season from the model as the mnth variable give us more insight into the data as there is more available inputs. 

## BOX-COX TRANSFORMATION

```{r model 2 atemp casual boxcox}
x <- subset(df, select = - c(season,temp))
x$casual <- x$casual+1
x$registered <- x$registered+1
bc<-boxcox(casual ~. - registered,data = x)
lambda <- bc$x[which.max(bc$y)]
model <- lm(((casual^lambda-1)/lambda) ~ . - registered, data = x)
summary(model)
plot(model)
```

After the Box-Cox transformation, the optimal lambda is 0.101. Even though visually from the graph the lambda = 0 value seems to be within the 95% CI, it is uncertain if the 0 value is actually in the interval. Therefore, the actual lambda of 0.101 will be used without approximation, consequently, the dependent variable will be (y^lambda - 1)/lambda (instead of log(y) if the lambda = 0).

After applying the transformation to the initial dataset model, it can also be seen that the adjusted R-squared, and the diagnostics seem to be improving, proving that the Box-Cox transformed model is a better fit to the data.

## VARIABLE SELECTION

```{r model 3 casual atemp LASSO}
x$casual <-(x$casual^lambda-1)/lambda
y <- x$casual
X <- subset(x, select = - c(casual,registered))
X <- as.matrix(X)
cross.validation <- cv.glmnet(X, y, alpha = 1,type.measure = "mse") 
optimal.lambda <- cross.validation$lambda.min
lm.lasso.optimal <-glmnet(X, y, 
                   lambda = optimal.lambda,
                   alpha = 1, #alpha=1 is the LASSO penalty
                   family = 'gaussian')
coef(lm.lasso.optimal)
X<- subset(X,select = - weathersit)
summary(lm(y~X))
plot(lm(y~X))
```

```{r press}
PRESS(lm(y~X))
```