---
title: "DATA 410 Project"
author: "Kyle Nelwan"
date: "01/04/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MPV)
library(MASS)
library(bestglm)
library(glmnet)
library(regclass)
library(car)
```

```{r initialize}
df <- read.csv("hour.csv")
df<- subset(df, select = -c(instant,cnt,dteday))


```

```{r model 1 casual}
model1_casual <- lm(casual ~. - registered, data = df)
summary(model1_casual)
#par(mfrow=c(2,2))
plot(model1_casual)

```
First, we regress the number of casual users with all the available variables. At first glance, we could see that most of variables are highly significant. However, notice that our adjusted R-squared is less than 0.5. Therefore, even though our model is significant, it can only explain less than half of the data.

The residuals vs fitted diagnostic plot has a clear pattern. Our data set seems to have a quadratic relationship.
The Normal Q-Q plot shows that our model is highly right skewed. This is a huge problem for us. Notice that those rightly skewed data have a very large value of standardized residuals. Hence, the this original linear model is flawed.
The residuals vs leverage plot tells us that none of our data are influential points but have high residuals as stated before.



```{r model 1 registered}
model1_regis <- lm(registered ~. - casual, data = df)
summary(model1_regis)
# par(mfrow=c(2,2))
plot(model1_regis)
```
The summary of this model shows us that most of our variables are significant enough (5% significance level). However, this  model has a low adjusted R-squared value at 0.335. Hence, this model is great at explaining the data we have.
on further inspection, our data may have a quadratic or a decaying relationship. This is shown in the residuals vs fitted plot.  
The Normal Q-Q plot shows that our data set have a light left tail and a heavy right tail. Hence, we can conclude that our data set is rightly skewed. Note that a lot of our data are beyond the 2 standardized residuals range. Therefore, we have a problem with our current linear model. 
Similar with the previous model, this model does not have any influence points but have high residuals value.


```{r model 2 casual quadratic initial assessment}

x <- df

x$season2 <- x$season^2
x$mnth2 <- x$mnth^2
x$hr2 <- x$hr^2
x$weekday2 <- x$weekday^2
x$weathersit2 <- x$weathersit^2
x$temp2 <- x$temp^2
x$atemp2 <- x$atemp^2
x$hum2 <- x$hum^2
x$windspeed2 <- x$windspeed^2

model <- lm(casual~. -casual - registered - yr, data = x)
summary(model)
plot(model)

```
```{r model 2 registered quadratic initial assessment}}
model <-lm(registered~. -casual - registered , data = x)
summary(model)
plot(model)
```


```{r multicollinearity check}
VIF(model1_casual)
```
We can see that there exist a high VIF value between temp and atemp. This make sense since both variable indicates the value of the hourly temperature. Therefore, we need to remove one of those variables.
Also, there seems to be a relatively high VIF value between season and month. This make sense since certain months always have the same season. Hence, we remove season from the model as the mnth variable give us more insight into the data as there is more available inputs. 

```{r multicollinearity solution}
VIF(lm(casual ~. - registered - temp - season, data = df))
VIF(lm(casual ~. - registered - atemp - season, data = df))

```



```{r model casual without temp variable selection LASSO}
x <- df

x$season2 <- x$season^2
x$mnth2 <- x$mnth^2
x$hr2 <- x$hr^2
x$weekday2 <- x$weekday^2
x$weathersit2 <- x$weathersit^2
x$temp2 <- x$temp^2
x$atemp2 <- x$atemp^2
x$hum2 <- x$hum^2
x$windspeed2 <- x$windspeed^2

y <- subset(x, select = c(casual,registered))
x<-subset(x,select = -c(casual,temp,season,registered,temp2,season2))

cross.validation <- cv.glmnet(as.matrix(x), y$casual, alpha = 1,type.measure = "mse") 

optimal.lambda <- cross.validation$lambda.min


lm.lasso.optimal <-glmnet(x, casual, 
                   lambda = optimal.lambda,
                   alpha = 1, #alpha=1 is the LASSO penalty
                   family = 'gaussian')

coef(lm.lasso.optimal)


summary(lm(casual~.,data=x))
plot(lm(casual~.,data=x))
```

```{r model casual without atemp variable selection LASSO}
x<-subset(df,select = -c(casual,atemp,season,registered))
cross.validation <- cv.glmnet(as.matrix(x), casual, alpha = 1,type.measure = "mse") 

optimal.lambda <- cross.validation$lambda.min


lm.lasso.optimal <-glmnet(x, casual, 
                   lambda = optimal.lambda,
                   alpha = 1, #alpha=1 is the LASSO penalty
                   family = 'gaussian')

x <- subset(x,select = -windspeed)
coef(lm.lasso.optimal)
summary(lm(casual~as.matrix(x)))
plot(lm(casual~as.matrix(x)))
```


```{r model registred without temp variable selection LASSO}
x<-subset(df,select = -c(casual,temp,season,registered))
cross.validation <- cv.glmnet(as.matrix(x), registered, alpha = 1,type.measure = "mse") 

optimal.lambda <- cross.validation$lambda.min


lm.lasso.optimal <-glmnet(x, registered, 
                   lambda = optimal.lambda,
                   alpha = 1, #alpha=1 is the LASSO penalty
                   family = 'gaussian')

coef(lm.lasso.optimal)
summary(lm(registered~as.matrix(x)))
plot(lm(registered~as.matrix(x)))
```

```{r model registered without atemp variable selection LASSO}
x<-subset(df,select = -c(casual,atemp,season,registered))
cross.validation <- cv.glmnet(as.matrix(x), registered, alpha = 1,type.measure = "mse") 

optimal.lambda <- cross.validation$lambda.min


lm.lasso.optimal <-glmnet(x, registered, 
                   lambda = optimal.lambda,
                   alpha = 1, #alpha=1 is the LASSO penalty
                   family = 'gaussian')

coef(lm.lasso.optimal)

summary(lm(registered~as.matrix(x)))
plot(lm(registered~as.matrix(x)))
```

```{r test}

```